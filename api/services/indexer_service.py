"""Copyright (c) 2025 DFlexy"""
"""https://github.com/DFlexy"""

import logging
from typing import List, Dict, Optional
from app.config import Config
from scraper import (
    create_scraper,
    available_scraper_types,
    normalize_scraper_type,
)
from core.enrichers.torrent_enricher import TorrentEnricher
from core.filters.query_filter import QueryFilter
from core.processors.torrent_processor import TorrentProcessor

logger = logging.getLogger(__name__)

SCRAPER_NUMBER_MAP = {
    "1": "starck",
    "2": "rede",
    "3": "baixafilmes",
    "4": "tfilme",
    "5": None,  # Removido (vaca)
    "6": "comand",
    "7": "bludv",
    "8": "nerd",
}


# Retorna apenas os IDs válidos (não None) do mapeamento
# Usado para garantir que IDs removidos não apareçam em nenhum lugar
def get_valid_scraper_ids() -> Dict[str, str]:
    return {k: v for k, v in SCRAPER_NUMBER_MAP.items() if v is not None}


class IndexerService:
    def __init__(self):
        self.enricher = TorrentEnricher()
        self.processor = TorrentProcessor()
    
    # Busca torrents por query
    def search(self, scraper_type: str, query: str, use_flaresolverr: bool = False, filter_results: bool = False) -> tuple[List[Dict], Optional[Dict]]:
        scraper = create_scraper(scraper_type, use_flaresolverr=use_flaresolverr)
        
        filter_func = None
        if filter_results and query:
            filter_func = QueryFilter.create_filter(query)
        
        # Obtém estatísticas ANTES de processar (garante que não serão sobrescritas)
        filter_stats = None
        
        torrents = scraper.search(query, filter_func=filter_func)
        
        # Obtém estatísticas imediatamente após o enriquecimento (evita race condition)
        if hasattr(scraper, '_enricher') and hasattr(scraper._enricher, '_last_filter_stats'):
            # Faz cópia das estatísticas para evitar que sejam sobrescritas por outras requisições
            stats = scraper._enricher._last_filter_stats
            if stats:
                filter_stats = {
                    'total': stats.get('total', 0),
                    'filtered': stats.get('filtered', 0),
                    'approved': stats.get('approved', 0),
                    'scraper_name': stats.get('scraper_name', '')
                }
        
        self.processor.sanitize_torrents(torrents)
        self.processor.remove_internal_fields(torrents)
        self.processor.sort_by_date(torrents)
        
        return torrents, filter_stats
    
    # Retorna as estatísticas do último filtro aplicado
    def get_last_filter_stats(self):
        return self.enricher._last_filter_stats if hasattr(self.enricher, '_last_filter_stats') else None
    
    # Obtém torrents de uma página
    def get_page(self, scraper_type: str, page: str = '1', use_flaresolverr: bool = False, is_test: bool = False) -> tuple[List[Dict], Optional[Dict]]:
        scraper = create_scraper(scraper_type, use_flaresolverr=use_flaresolverr)
        
        max_links = None
        if is_test:
            max_links = Config.EMPTY_QUERY_MAX_LINKS if Config.EMPTY_QUERY_MAX_LINKS > 0 else None
        
        torrents = scraper.get_page(page, max_items=max_links)
        
        # Obtém estatísticas imediatamente após o enriquecimento (evita race condition)
        filter_stats = None
        if hasattr(scraper, '_enricher') and hasattr(scraper._enricher, '_last_filter_stats'):
            # Faz cópia das estatísticas para evitar que sejam sobrescritas por outras requisições
            stats = scraper._enricher._last_filter_stats
            if stats:
                filter_stats = {
                    'total': stats.get('total', 0),
                    'filtered': stats.get('filtered', 0),
                    'approved': stats.get('approved', 0),
                    'scraper_name': stats.get('scraper_name', '')
                }
        
        self.processor.sanitize_torrents(torrents)
        self.processor.remove_internal_fields(torrents)
        
        if not (is_test and Config.EMPTY_QUERY_MAX_LINKS > 0):
            self.processor.sort_by_date(torrents)
        
        return torrents, filter_stats
    
    # Obtém informações dos scrapers disponíveis
    def get_scraper_info(self) -> Dict:
        types_info = available_scraper_types()
        sites_dict = {
            scraper_type: meta.get('default_url')
            for scraper_type, meta in types_info.items()
            if meta.get('default_url')
        }
        
        return {
            'configured_sites': sites_dict,
            'available_types': list(types_info.keys()),
            'types_info': types_info
        }
    
    # Valida tipo de scraper e retorna tipo normalizado
    def validate_scraper_type(self, scraper_type: str) -> tuple[bool, Optional[str]]:
        if scraper_type in SCRAPER_NUMBER_MAP:
            mapped_type = SCRAPER_NUMBER_MAP[scraper_type]
            # Se o mapeamento for None (scraper removido), retorna inválido
            # Isso permite remover scrapers sem precisar reajustar IDs no prowlarr.yml
            if mapped_type is None:
                return False, None
            scraper_type = mapped_type
        
        types_info = available_scraper_types()
        normalized_type = normalize_scraper_type(scraper_type)
        
        if normalized_type not in types_info:
            return False, None
        
        return True, normalized_type

